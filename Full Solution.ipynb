{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Kaggle Airbnb Dataset</h1>\n",
    "<h2>Capstone Project 1</h2>\n",
    "<h2>Springboard Data Science Career Track</h2>\n",
    "\n",
    "This notebook is a full demonstration of machine learning skills, including data wrangling, feature engineering, model training and hyperparameter selection.\n",
    "\n",
    "The goal of the Airbnb Contest from 2015 is to predict each user's top five most likely destinations, but the distribution of destinations is very skewedâ€”most users either don't book at all (\"NDF\") or they book in the US. This is a multiclassifier problem with <i>imbalanced classes.</i> The goal, then, is to maximize accuracy of results across the other three less probable destination countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Wrangling & Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('data/raw/train_users_2.csv')\n",
    "df_test = pd.read_csv('data/raw/test_users.csv')\n",
    "target = df_train['country_destination'].values\n",
    "df_train = df_train.drop(['country_destination'], axis=1)\n",
    "id_test = df_test['id']\n",
    "trainsize = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and test data, to wrangle both datasets at once\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "# Remove date first booking because it does not appear in the test data\n",
    "df_all = df_all.drop(['date_first_booking'], axis=1)\n",
    "# Fill in null values\n",
    "df_all = df_all.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Datetime Objects</h3>\n",
    "\n",
    "To break the features \"date_account_created\" and \"timestamp_first_active\" into day, month, and year, first convert each of them into datetime objects, then create three new features, then delete the originals.\n",
    "\n",
    "It would have been possible to break these dates down into any number of features. In a previous exploration, I tried using just the week of the year. (i.e. Jan 1-7 is first week, 8-14 is second week...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dac = pd.to_datetime(df_all['date_account_created'])\n",
    "df_all['dac_year'] = [entry.year for entry in dac]\n",
    "df_all['dac_month'] = [entry.month for entry in dac]\n",
    "df_all['dac_day'] = [entry.day for entry in dac]\n",
    "df_all['dac_weekday'] = [entry.dayofweek for entry in dac]\n",
    "df_all = df_all.drop(['date_account_created'], axis=1)\n",
    "\n",
    "tfa = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "df_all['tfa_year'] = [entry.year for entry in tfa]\n",
    "df_all['tfa_month'] = [entry.month for entry in tfa]\n",
    "df_all['tfa_day'] = [entry.day for entry in tfa]\n",
    "df_all['tfa_weekday'] = [entry.dayofweek for entry in tfa]\n",
    "df_all = df_all.drop(['timestamp_first_active'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Age</h3>\n",
    "\n",
    "Age is the dirtiest feature. There are missing values, illogical ages (under 18, over 100), and birth years and current years incorrectly entered as ages. We will replace each error with a different dummy number, well separated from the correct values and from each other dummy number.\n",
    "\n",
    "All ages are left to form a linear term. It would also be possible to make categories out of the ages, then keep those categories linear or one hot encode the categories. Doing so loses a bit of information, but may make for easier-to-interpret results. (If there's a particular age group you think would stand out, it'd be easier to check via one-hot encoding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = df_all.age.values\n",
    "\n",
    "# Convert birth years to ages:\n",
    "# (Data was collected in 2014, so 2014 minus birth year = age)\n",
    "ages = np.where(np.logical_and(ages < 2000, ages > 1900), 2014-ages, ages)\n",
    "# Create a dummy number for all ages below 18:\n",
    "ages = np.where(np.logical_and(ages < 18, ages > 0), 4, ages)\n",
    "# Create a dummy number for all entries with current year instead of age:\n",
    "ages = np.where(np.logical_and(ages < 2016, ages > 2010), -5, ages)\n",
    "# Create a dummy number for all entries allegedly older than 100:\n",
    "ages = np.where(ages > 99, 110, ages)\n",
    "# Store to age\n",
    "df_all['age'] = ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One Hot Encoding</h3>\n",
    "\n",
    "Machine Learning algorithms can only handle numerical inputs, but you can still pass categorical data to the algorithm by one hot encoding. This involves creating a new dummy feature for each category, then assigning a 1 to that feature of the original feature was that category (hot) or a 0 if it wasn't.\n",
    "\n",
    "For example, \"Browser: Firefox\" can be coded to \"Is_Firefox: 1\", \"Is_Chrome: 0\", \"Is_Safari: 0\", and \"Is_Edge: 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummiescols = ['gender', 'signup_method', 'signup_flow', 'language',\n",
    "               'affiliate_channel', 'affiliate_provider',\n",
    "               'first_affiliate_tracked', 'signup_app', 'first_device_type',\n",
    "               'first_browser']\n",
    "df_all = pd.get_dummies(df_all, prefix=dummiescols, columns=dummiescols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sessions Data: Split-Apply-Combine</h3>\n",
    "\n",
    "The Airbnb Data Set has one file containing the user data (dealt with above) and another file entirely for sessions data. No sessions data is given for test users, only training users. Because of this, we can only use the sessions data to help us sharpen our picture of our training set. However, by including sessions data, we can append a lot of new features to the user training data, hoping to find inherent commonalities between users that carry over to their ID data.\n",
    "\n",
    "We want to create summary statistics about the sessions users logged on the site: what kind of actions they performed, how often they performed them, and how long they spent between actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.read_csv('data/raw/sessions.csv')\n",
    "# Rename to be exactly the right name for our merge\n",
    "df_sess['id'] = df_sess['user_id']\n",
    "df_sess = df_sess.drop(['user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan with 'NAN' in all columns except id and seconds\n",
    "fillcols = ['action', 'action_type', 'action_detail', 'device_type']\n",
    "df_sess[fillcols] = df_sess[fillcols].fillna('NAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 of 135483\n",
      "Processing 10000 of 135483\n",
      "Processing 20000 of 135483\n",
      "Processing 30000 of 135483\n",
      "Processing 40000 of 135483\n",
      "Processing 50000 of 135483\n",
      "Processing 60000 of 135483\n",
      "Processing 70000 of 135483\n",
      "Processing 80000 of 135483\n",
      "Processing 90000 of 135483\n",
      "Processing 100000 of 135483\n",
      "Processing 110000 of 135483\n",
      "Processing 120000 of 135483\n",
      "Processing 130000 of 135483\n"
     ]
    }
   ],
   "source": [
    "grouped = df_sess.groupby(['id'])\n",
    "# Initialize an empty list to append each id's feature sets\n",
    "id_features = []\n",
    "count = 0\n",
    "last = len(grouped)\n",
    "for key, table in grouped:\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"Processing {count} of {last}\") \n",
    "    # Initialize an empty list to append features\n",
    "    list = []\n",
    "    # Append the user id to the list\n",
    "    list.append(key)\n",
    "    # Append the number of actions taken\n",
    "    list.append(len(table))\n",
    "    # Append the number of unique actions taken\n",
    "    list.append(table['action'].nunique())\n",
    "    # Append the number of unique action details taken\n",
    "    list.append(table['action_detail'].nunique())\n",
    "    # Append the number of unique device types used\n",
    "    list.append(table['device_type'].nunique())\n",
    "    # Append the sum of seconds elapsed\n",
    "    list.append(np.sum(table['secs_elapsed']))\n",
    "    # Append the mean number of seconds elapsed\n",
    "    list.append(np.mean(table['secs_elapsed']))\n",
    "    # Append the standard deviation of seconds elapsed\n",
    "    list.append(np.std(table['secs_elapsed']))\n",
    "    \n",
    "    # Append this data as a row of id_features.\n",
    "    id_features.append(list)\n",
    "    count += 1\n",
    "   \n",
    "# Create the aggregate sessions table\n",
    "new_features = ['id', 'action_count', 'unique_action_count',\n",
    "                'unique_detail_count', 'device_type_count',\n",
    "                'sum_secs', 'mean_secs', 'stdev_secs']\n",
    "\n",
    "id_features = np.array(id_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess_agg = pd.DataFrame(id_features, columns=new_features)\n",
    "# Replace infinity and negative infinity with NaN; much easier to handle\n",
    "df_sess_agg = df_sess_agg.replace([np.inf, -np.inf], np.nan)\n",
    "df_sess_agg = df_sess_agg.fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d\\Anaconda3\\envs\\capstone1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Merge sessions data onto user feature data\n",
    "df_all = pd.merge(df_all, df_sess_agg, on='id', how='left')\n",
    "df_all = df_all.fillna(-2) # Summary statistics for users with no session data\n",
    "\n",
    "# Split back to training and testing data\n",
    "df_train = df_all[:trainsize]\n",
    "# Add target row back\n",
    "df_train['country_destination'] = target\n",
    "# One more check to make sure no bad values\n",
    "df_train = df_train.replace([np.inf, -np.inf], np.nan)\n",
    "df_train = df_train.fillna(-2)\n",
    "\n",
    "# Repeat for test row\n",
    "df_test = df_all[trainsize:]\n",
    "df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
    "df_test = df_test.fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data before proceeding\n",
    "\n",
    "df_train.to_csv('data/processed/train_users_2.csv', index=False)\n",
    "df_test.to_csv('data/processed/test_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Training & Hyperparameter Selection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train['country_destination'].values\n",
    "df_train = df_train.drop(['id', 'country_destination'], axis=1)\n",
    "id_test = df_test['id']\n",
    "df_test = df_test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test, convert target classes to different numbers\n",
    "# So the algorithm can process it\n",
    "X_train = df_train.to_numpy()\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(target)\n",
    "X_test = df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>XGBoost Classifier</h3>\n",
    "\n",
    "Here we are creating the algorithm that will classify our data (i.e. try to predict who is going where). XGBoost uses gradient boosting of random forests. We create a weak decision tree, evaluate its predictions combined with the predictions of each previously generated tree, and then create the next tree <i>with special emphasis on the data points that the forest misclassified this time.</i> After the model is trained, we'll use the whole random forest to predict data it's never seen before, and see how well it does.\n",
    "\n",
    "We are choosing the objective 'multi:softprob' because we want the top five choices, not just the top choice. This objective will give us a table of the probabilities of belonging to a category, then we'll cut it to the top five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5,\n",
    "                    colsample_by_tree=0.5, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Selection</h3>\n",
    "\n",
    "XGBClassifier takes a lot of parameters under consideration. How many trees in the forest? How quickly does the next tree improve over the previous one's mistakes? How many questions can any one decision tree ask when trying to classify? \n",
    "\n",
    "There's no objective answer to any of these questions that works in every situation, so we will build the model multiple times and check to see what the right answers are for this problem. We do this by cross-validating: training the model on Third A and Third B of the training set (letting it see the answers), testing on Third C (withholding the answers and making it guess), then repeat testing on Third A (giving it B&C) and Third B (giving it A&C). We take the model that performs the best under cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the parameters to tune\n",
    "# Note by this grid, we are making 21 different models\n",
    "param_grid = {'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.3, 0.2, 0.1],\n",
    "              'max_depth': [5, 6, 7]}\n",
    "\n",
    "# With 3-fold cross-validation, it comes to 81\n",
    "clf = GridSearchCV(xgb, param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_by_tree=0.5,\n",
       "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.3,\n",
       "       max_delta_step=0, max_depth=6, min_child_weight=1, missing=None,\n",
       "       n_estimators=25, n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=0, silent=True, subsample=0.5),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [25, 50, 100], 'learning_rate': [0.3, 0.2, 0.1], 'max_depth': [5, 6, 7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 25}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the hyperparameters that worked the best\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_by_tree=0.5,\n",
       "       colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=25, n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=0, silent=True, subsample=0.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the model with the best parameters and train it one more time\n",
    "\n",
    "xgb_best = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5,\n",
    "                    colsample_by_tree=0.5, seed=0)\n",
    "\n",
    "xgb_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "xgb_best.save_model('airbnbXGB.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to predict the test set\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 5 classes with the highest probabilities,\n",
    "# return them to classes rather than just numbers,\n",
    "# and export them to a list\n",
    "ids = [] # List of ids\n",
    "countries = [] # List of countries\n",
    "for i in range(len(id_test)):\n",
    "    this_id = id_test.iloc[i]\n",
    "    ids += [this_id] * 5\n",
    "    countries += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "    \n",
    "# Generate submission. This could be uploaded to Kaggle\n",
    "submission = pd.DataFrame(np.column_stack((ids, countries)),\n",
    "                          columns=['id', 'country'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "We now have a model that predicts the Top 5 most likely destinations a new Airbnb user will go for their first booking. If you upload \"submission.csv\" to Kaggle, you'll get a Private Score of <b>.87252</b>, which would have put your submission in the top 25% of final submissions for this competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
